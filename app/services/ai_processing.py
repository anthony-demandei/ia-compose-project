from typing import Dict, List, Optional, Any
import json
import re
from openai import AsyncOpenAI
from app.models.base import DiscoveryStage, ValidationResult
from app.models.session import DiscoverySession, Message
from app.core.validation_engine import ValidationEngine
from app.templates.system_prompts import DemandeiDiscoveryPO
from app.utils.pii_safe_logging import get_pii_safe_logger
from app.utils.token_optimizer import TokenOptimizer, optimize_messages_for_api

logger = get_pii_safe_logger(__name__)


class AIResponse:
    def __init__(self, content: str, stage: DiscoveryStage, metadata: Optional[Dict] = None):
        self.content = content
        self.stage = stage
        self.metadata = metadata or {}


class AIProcessingEngine:
    """
    Engine de processamento de IA que gera respostas contextuais,
    faz perguntas de esclarecimento e extrai requisitos das conversas.
    """

    def __init__(self, api_key: str, model: str = None):
        from app.utils.config import get_settings

        settings = get_settings()
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model or settings.openai_model
        self.validation_engine = ValidationEngine()
        self.token_optimizer = TokenOptimizer(model)

        # Configura√ß√µes de temperatura por tipo de opera√ß√£o
        self.temperatures = {
            "validation": 0.3,
            "ideation": 0.6,
            "extraction": 0.4,
            "questions": 0.7,
        }

        self._initialize_stage_prompts()

    def _initialize_stage_prompts(self):
        """Inicializa os prompts espec√≠ficos para cada est√°gio usando o sistema avan√ßado."""
        # Obter prompt principal do sistema
        self.main_system_prompt = DemandeiDiscoveryPO.get_main_system_prompt()

        # Obter prompts espec√≠ficos por est√°gio
        self.stage_prompts = {}
        stage_configs = DemandeiDiscoveryPO.get_stage_prompts()

        for stage, config in stage_configs.items():
            self.stage_prompts[stage] = {
                "system": f"{self.main_system_prompt}\n\n{config['system']}",
                "questions": config["questions"],
                "validation_prompt": config.get("validation_prompt", ""),
            }

    async def generate_response(
        self,
        session: DiscoverySession,
        user_message: str,
        uploaded_files: Optional[List[Dict[str, Any]]] = None,
    ) -> AIResponse:
        """
        Gera resposta contextual baseada no est√°gio atual e hist√≥rico da conversa.
        """
        try:
            current_stage = session.current_stage
            stage_config = self.stage_prompts.get(current_stage)

            if not stage_config:
                logger.error("Configura√ß√£o n√£o encontrada para est√°gio: {}", current_stage)
                return AIResponse(
                    content="Desculpe, houve um erro interno. Por favor, tente novamente.",
                    stage=current_stage,
                )

            # Construir contexto da conversa
            conversation_context = self._build_conversation_context(session)

            # Construir contexto de arquivos se fornecido
            files_context = ""
            if uploaded_files:
                files_context = await self._build_files_context(uploaded_files)

            # Validar est√°gio atual
            validation_result = self.validation_engine.validate_stage(
                current_stage, session.requirements
            )

            # Adicionar instru√ß√µes de formata√ß√£o e anti-repeti√ß√£o
            formatting_instructions = f"""
REGRAS CR√çTICAS PARA PERGUNTAS:

1. **NUNCA REPITA** perguntas sobre dados j√° coletados (veja DADOS J√Å COLETADOS acima)
2. **AVANCE AUTOMATICAMENTE** se j√° tem 80%+ das informa√ß√µes necess√°rias  
3. **ACEITE RESPOSTAS CURTAS** como "web", "n√£o sei", "azul"
4. **N√ÉO INSISTA** em detalhes se o usu√°rio deu respostas diretas
5. **M√ÅXIMO 3 PERGUNTAS** por turno - seja seletivo e estrat√©gico

FORMATA√á√ÉO DAS PERGUNTAS:
**1. Primeira pergunta?**
Explica√ß√£o breve se necess√°rio.

**2. Segunda pergunta?** 
Explica√ß√£o breve se necess√°rio.

**3. Terceira pergunta?**
Explica√ß√£o breve se necess√°rio.

Nunca coloque m√∫ltiplas perguntas numeradas na mesma linha.

VALIDA√á√ÉO DO EST√ÅGIO ATUAL: 
Score: {validation_result.completeness_score:.1%} | Faltando: {', '.join(validation_result.missing_items[:3])}
"""

            # Gerar prompt baseado no contexto
            messages = [
                {"role": "system", "content": stage_config["system"]},
                {"role": "system", "content": formatting_instructions},
                {"role": "system", "content": f"CONTEXTO DA CONVERSA:\n{conversation_context}"},
                {
                    "role": "system",
                    "content": f"VALIDA√á√ÉO ATUAL:\n{self._format_validation_result(validation_result)}",
                },
            ]

            # Adicionar contexto de arquivos se dispon√≠vel
            if files_context:
                messages.append(
                    {"role": "system", "content": f"ARQUIVOS ENVIADOS:\n{files_context}"}
                )

            # Adicionar mensagem do usu√°rio
            messages.append({"role": "user", "content": user_message})

            # Preparar mensagens com suporte multimodal
            if uploaded_files:
                messages = await self._prepare_multimodal_messages(messages, uploaded_files)

            # Otimizar mensagens para economizar tokens
            optimized_messages, token_usage = optimize_messages_for_api(messages, self.model)

            # Log de otimiza√ß√£o se houve economia significativa
            if token_usage.optimization_savings > 100:
                logger.info(
                    "Otimiza√ß√£o de tokens aplicada: {} tokens economizados",
                    token_usage.optimization_savings,
                )

            # Chamar OpenAI com structured output quando necess√°rio
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=optimized_messages,
                temperature=self.temperatures["ideation"],
                max_tokens=1000,
            )

            ai_content = response.choices[0].message.content

            # Formatar perguntas numeradas se houver
            ai_content = self._format_numbered_questions(ai_content)

            # Extrair dados da resposta do usu√°rio
            await self._extract_and_update_requirements(session, user_message, current_stage)

            # Verificar se deve avan√ßar automaticamente ap√≥s a atualiza√ß√£o
            should_advance = await self.should_advance_stage(session)

            # Se pode avan√ßar e n√£o est√° no √∫ltimo est√°gio, adicionar mensagem de avan√ßo
            if should_advance and current_stage != DiscoveryStage.FINALIZE_DOCS:
                next_stage = self._get_next_stage(current_stage)
                if next_stage:
                    ai_content += f"\n\n‚úÖ **√ìtimo! Temos informa√ß√µes suficientes sobre {current_stage.value}.**\n\nüöÄ **Vamos para o pr√≥ximo t√≥pico: {next_stage.value}**"

            return AIResponse(
                content=ai_content,
                stage=current_stage,
                metadata={
                    "validation_score": validation_result.completeness_score,
                    "missing_items": validation_result.missing_items,
                    "can_advance": should_advance,
                    "should_advance": should_advance,
                },
            )

        except Exception as e:
            logger.error("Erro ao gerar resposta IA: {}", str(e))
            return AIResponse(
                content="Desculpe, houve um erro ao processar sua mensagem. Por favor, tente novamente.",
                stage=session.current_stage,
            )

    async def generate_clarifying_questions(self, stage: DiscoveryStage) -> List[str]:
        """Gera perguntas de esclarecimento para um est√°gio espec√≠fico."""
        stage_config = self.stage_prompts.get(stage)
        if stage_config and "questions" in stage_config:
            return stage_config["questions"]
        return ["Pode me dar mais detalhes sobre este aspecto?"]

    async def extract_requirements(self, conversation: List[Message]) -> Dict[str, Any]:
        """
        Extrai requisitos estruturados de uma conversa.
        """
        try:
            # Construir contexto da conversa
            conversation_text = "\n".join([f"{msg.sender}: {msg.content}" for msg in conversation])

            extraction_prompt = f"""
            Analise a seguinte conversa e extraia informa√ß√µes estruturadas:
            
            {conversation_text}
            
            Retorne um JSON com as informa√ß√µes extra√≠das, seguindo esta estrutura:
            {{
                "business_context": {{
                    "objetivo": "texto extra√≠do",
                    "personas": ["persona1", "persona2"],
                    "kpis": ["kpi1", "kpi2"],
                    "riscos_principais": ["risco1", "risco2"]
                }},
                "confidence_score": 0.8
            }}
            
            Retorne apenas o JSON, sem explica√ß√µes adicionais.
            """

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Voc√™ √© um especialista em extra√ß√£o de requisitos. Retorne apenas JSON v√°lido.",
                    },
                    {"role": "user", "content": extraction_prompt},
                ],
                temperature=self.temperatures["extraction"],
                max_tokens=2000,
            )

            extracted_data = json.loads(response.choices[0].message.content)
            return extracted_data

        except Exception as e:
            logger.error("Erro na extra√ß√£o de requisitos: {}", str(e))
            return {"error": str(e)}

    def _build_conversation_context(self, session: DiscoverySession) -> str:
        """Constr√≥i contexto da conversa para a IA."""
        recent_messages = session.get_conversation_history(limit=10)

        context_parts = [
            f"EST√ÅGIO ATUAL: {session.current_stage.value}",
            f"TOTAL DE MENSAGENS: {len(session.messages)}",
            f"SESS√ÉO CRIADA EM: {session.created_at}",
            "",
        ]

        # Adicionar dados j√° coletados para evitar repeti√ß√µes
        stage_data = session.requirements.get_stage_data(session.current_stage)
        if stage_data:
            context_parts.append("DADOS J√Å COLETADOS NESTE EST√ÅGIO:")
            stage_dict = stage_data.dict()

            collected_data = []
            for field, value in stage_dict.items():
                if value and value != [] and value != {}:
                    if isinstance(value, str) and len(value) > 0:
                        collected_data.append(f"‚úÖ {field.replace('_', ' ')}: {value}")
                    elif isinstance(value, list) and len(value) > 0:
                        collected_data.append(f"‚úÖ {field.replace('_', ' ')}: {', '.join(value)}")
                    elif isinstance(value, dict) and len(value) > 0:
                        collected_data.append(f"‚úÖ {field.replace('_', ' ')}: {str(value)}")

            # Adicionar t√≥picos coletados dos outros est√°gios tamb√©m
            all_stages_data = []
            for stage in DiscoveryStage:
                if stage != session.current_stage and stage != DiscoveryStage.FINALIZE_DOCS:
                    other_data = session.requirements.get_stage_data(stage)
                    if other_data:
                        other_dict = other_data.dict()
                        for field, value in other_dict.items():
                            if value and value != [] and value != {}:
                                if isinstance(value, str) and len(value) > 0:
                                    all_stages_data.append(
                                        f"‚Ä¢ {stage.value}: {field.replace('_', ' ')} = {value}"
                                    )

            context_parts.extend(collected_data)

            if all_stages_data:
                context_parts.append("")
                context_parts.append("DADOS COLETADOS EM OUTROS EST√ÅGIOS:")
                context_parts.extend(all_stages_data[:10])  # Limite para n√£o poluir muito

            context_parts.append("")
            context_parts.append(
                "üö® ANTI-REPETI√á√ÉO: NUNCA pergunte novamente sobre dados j√° coletados acima!"
            )
            context_parts.append("")

        context_parts.append("HIST√ìRICO RECENTE:")
        for msg in recent_messages:
            context_parts.append(f"{msg.sender}: {msg.content[:200]}...")

        return "\n".join(context_parts)

    def _format_validation_result(self, validation: ValidationResult) -> str:
        """Formata resultado da valida√ß√£o para contexto da IA."""
        return f"""
        COMPLETUDE: {validation.completeness_score:.1%}
        STATUS: {'COMPLETO' if validation.is_complete else 'INCOMPLETO'}
        ITENS FALTANTES: {', '.join(validation.missing_items)}
        SUGEST√ïES: {', '.join(validation.suggestions)}
        """

    def _format_numbered_questions(self, content: str) -> str:
        """
        Formata perguntas numeradas para ficarem organizadas uma abaixo da outra.
        """
        # Substitui perguntas numeradas que est√£o inline (na mesma linha)
        # Procura por padr√µes como: **1. pergunta** texto **2. pergunta**

        # Primeiro, procura por perguntas numeradas em negrito
        pattern_bold = r"\*\*(\d+)\.\s*([^*]+?)\*\*"
        matches = list(re.finditer(pattern_bold, content))

        if matches:
            # Se encontrou perguntas numeradas em negrito
            new_content = content

            # Processa de tr√°s para frente para n√£o bagun√ßar os √≠ndices
            for match in reversed(matches):
                num = match.group(1)
                question = match.group(2)

                # Verifica se j√° est√° em uma nova linha
                start_pos = match.start()

                # Verifica o que vem antes
                before_text = new_content[:start_pos]
                after_text = new_content[match.end() :]

                # Se n√£o est√° no in√≠cio de uma linha, adiciona quebra
                if before_text and not before_text.endswith("\n"):
                    # Adiciona duas quebras antes
                    formatted = f"\n\n**{num}. {question}**"
                else:
                    formatted = f"**{num}. {question}**"

                # Se tem texto logo depois (n√£o √© quebra de linha), adiciona quebra
                if after_text and not after_text.startswith("\n"):
                    formatted = formatted + "\n"

                new_content = before_text + formatted + after_text

            content = new_content

        # Tamb√©m procura por perguntas sem negrito mas numeradas
        pattern_simple = r"(?<!\*\*)(\d+)\.\s+([A-Z][^.?!]*[.?!])"

        def replace_numbered(match):
            num = match.group(1)
            question = match.group(2)
            return f"\n\n**{num}. {question}**\n"

        # Aplica a substitui√ß√£o para perguntas simples numeradas
        content = re.sub(pattern_simple, replace_numbered, content)

        # Remove m√∫ltiplas quebras de linha consecutivas (mais de 2)
        content = re.sub(r"\n{3,}", "\n\n", content)

        # Remove espa√ßos em branco extras no in√≠cio e fim
        content = content.strip()

        return content

    async def _extract_and_update_requirements(
        self, session: DiscoverySession, user_message: str, stage: DiscoveryStage
    ):
        """
        Extrai informa√ß√µes da mensagem do usu√°rio e atualiza os requisitos.
        """
        try:
            # Tratamento especial para respostas numeradas
            numbered_pattern = r"(\d+)\.\s*([^0-9]+?)(?=\d+\.|$)"

            # Prompt espec√≠fico para o est√°gio BUSINESS_CONTEXT com os novos campos
            if stage == DiscoveryStage.BUSINESS_CONTEXT:
                extraction_prompt = f"""
                Analise esta mensagem do usu√°rio e extraia informa√ß√µes t√©cnicas do projeto:
                
                "{user_message}"
                
                Se a mensagem contiver respostas numeradas (1. resposta 2. resposta), mapeie para:
                1 = tipo_aplicativo (web, mobile, desktop)
                2 = nome_projeto
                3 = identidade_visual (cores, design)
                4 = stack_tecnologico
                5 = tipo_notificacoes
                
                Tamb√©m extraia:
                - descricao_basica: qualquer descri√ß√£o do que o app deve fazer
                - preferencias_design: cores, estilos mencionados
                
                Retorne APENAS um JSON v√°lido com os campos encontrados.
                Exemplo: {{"tipo_aplicativo": "web", "nome_projeto": "Max Finan√ßas", "identidade_visual": "verde e azul"}}
                """
            else:
                extraction_prompt = f"""
                Extraia informa√ß√µes espec√≠ficas para o est√°gio "{stage.value}" desta mensagem:
                
                "{user_message}"
                
                Retorne um JSON com os campos relevantes para este est√°gio.
                Se n√£o houver informa√ß√µes suficientes, retorne campos vazios.
                Seja preciso e extraia apenas o que est√° explicitamente mencionado.
                """

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Voc√™ √© um extrator de dados preciso. Retorne apenas JSON v√°lido.",
                    },
                    {"role": "user", "content": extraction_prompt},
                ],
                temperature=self.temperatures["extraction"],
                response_format={"type": "json_object"},
                max_tokens=500,
            )

            extracted_data = json.loads(response.choices[0].message.content)

            # Atualizar requisitos da sess√£o
            if extracted_data:
                session.requirements.update_stage_data(stage, extracted_data)
                logger.info("Requisitos atualizados para est√°gio {}: {}", stage, extracted_data)

        except Exception as e:
            logger.error("Erro ao extrair dados da mensagem: {}", str(e))

    async def should_advance_stage(self, session: DiscoverySession) -> bool:
        """Verifica se deve avan√ßar para o pr√≥ximo est√°gio."""
        validation = self.validation_engine.validate_stage(
            session.current_stage, session.requirements
        )
        return validation.completeness_score >= 0.8

    def _get_next_stage(self, current_stage: DiscoveryStage) -> Optional[DiscoveryStage]:
        """Retorna o pr√≥ximo est√°gio na sequ√™ncia."""
        stage_order = [
            DiscoveryStage.BUSINESS_CONTEXT,
            DiscoveryStage.USERS_AND_JOURNEYS,
            DiscoveryStage.FUNCTIONAL_SCOPE,
            DiscoveryStage.CONSTRAINTS_POLICIES,
            DiscoveryStage.NON_FUNCTIONAL,
            DiscoveryStage.TECH_PREFERENCES,
            DiscoveryStage.DELIVERY_BUDGET,
            DiscoveryStage.REVIEW_GAPS,
            DiscoveryStage.FINALIZE_DOCS,
        ]

        try:
            current_index = stage_order.index(current_stage)
            if current_index < len(stage_order) - 1:
                return stage_order[current_index + 1]
        except ValueError:
            pass

        return None

    async def generate_finalization_summary(self, session: DiscoverySession) -> str:
        """Gera resumo executivo para confirma√ß√£o final."""
        try:
            req = session.requirements

            # Extrair informa√ß√µes principais de cada est√°gio
            objetivo_principal = req.business_context.objetivo or "N√£o especificado"

            escopo_tecnico = []
            if req.functional_scope.features_must:
                escopo_tecnico.extend(req.functional_scope.features_must[:3])

            restricoes_principais = []
            if req.constraints_policies.lgpd_pii:
                restricoes_principais.append(f"LGPD: {str(req.constraints_policies.lgpd_pii)}")
            if req.non_functional.slos and req.non_functional.slos.get("latency"):
                restricoes_principais.append(
                    f"Lat√™ncia: <{req.non_functional.slos.get('latency')}ms"
                )

            cronograma_orcamento = []
            if req.delivery_budget.prazo_semanas:
                cronograma_orcamento.append(f"Prazo: {req.delivery_budget.prazo_semanas} semanas")
            if req.delivery_budget.budget:
                cronograma_orcamento.append(f"Or√ßamento: R$ {req.delivery_budget.budget:,.2f}")

            # Gerar prompt de finaliza√ß√£o personalizado
            finalization_prompt = DemandeiDiscoveryPO.get_finalization_prompt()
            finalization_prompt = finalization_prompt.replace(
                "[objetivo_principal]", objetivo_principal
            )
            finalization_prompt = finalization_prompt.replace(
                "[escopo_tecnico]", " ‚Ä¢ ".join(escopo_tecnico[:3])
            )
            finalization_prompt = finalization_prompt.replace(
                "[restricoes_principais]", " ‚Ä¢ ".join(restricoes_principais[:3])
            )
            finalization_prompt = finalization_prompt.replace(
                "[cronograma_orcamento]", " ‚Ä¢ ".join(cronograma_orcamento)
            )
            finalization_prompt = finalization_prompt.replace(
                "[riscos_principais]",
                " ‚Ä¢ ".join(
                    req.business_context.riscos_principais[:2]
                    if req.business_context.riscos_principais
                    else ["N√£o identificados"]
                ),
            )

            return finalization_prompt

        except Exception as e:
            logger.error("Erro ao gerar resumo de finaliza√ß√£o: {}", str(e))
            return DemandeiDiscoveryPO.get_finalization_prompt()

    async def generate_completion_message(self) -> str:
        """Gera mensagem de conclus√£o ap√≥s gera√ß√£o dos arquivos."""
        return DemandeiDiscoveryPO.get_completion_message()

    async def validate_stage_completion(
        self, stage: DiscoveryStage, session: DiscoverySession
    ) -> ValidationResult:
        """Valida se um est√°gio est√° completo usando prompt espec√≠fico."""
        try:
            stage_config = self.stage_prompts.get(stage)
            if not stage_config or not stage_config.get("validation_prompt"):
                # Fallback para valida√ß√£o padr√£o
                return self.validation_engine.validate_stage(stage, session.requirements)

            # Construir contexto das informa√ß√µes coletadas
            collected_info = self._build_stage_context(session, stage)

            # Usar prompt de valida√ß√£o espec√≠fico do est√°gio
            validation_prompt = stage_config["validation_prompt"].format(
                collected_info=collected_info
            )

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Voc√™ √© um validador rigoroso de requisitos. Retorne apenas JSON v√°lido.",
                    },
                    {"role": "user", "content": validation_prompt},
                ],
                temperature=self.temperatures["validation"],
                response_format={"type": "json_object"},
                max_tokens=500,
            )

            validation_data = json.loads(response.choices[0].message.content)

            return ValidationResult(
                is_complete=validation_data.get("is_complete", False),
                completeness_score=validation_data.get("completeness_score", 0.0),
                missing_items=validation_data.get("missing_items", []),
                suggestions=[],
            )

        except Exception as e:
            logger.error("Erro na valida√ß√£o avan√ßada do est√°gio {}: {}", stage, str(e))
            # Fallback para valida√ß√£o padr√£o
            return self.validation_engine.validate_stage(stage, session.requirements)

    def _build_stage_context(self, session: DiscoverySession, stage: DiscoveryStage) -> str:
        """Constr√≥i contexto espec√≠fico de um est√°gio para valida√ß√£o."""
        req = session.requirements

        stage_contexts = {
            DiscoveryStage.BUSINESS_CONTEXT: f"""
            Objetivo: {req.business_context.objetivo}
            Personas: {req.business_context.personas}
            KPIs: {req.business_context.kpis}
            Riscos: {req.business_context.riscos_principais}
            """,
            DiscoveryStage.USERS_AND_JOURNEYS: f"""
            Perfis: {req.users_and_journeys.perfis}
            Jornadas: {req.users_and_journeys.jornadas_criticas}
            Idiomas: {req.users_and_journeys.idiomas}
            Acessibilidade: {req.users_and_journeys.acessibilidade}
            """,
            DiscoveryStage.FUNCTIONAL_SCOPE: f"""
            Features Must: {req.functional_scope.features_must}
            Features Should: {req.functional_scope.features_should}
            Integra√ß√µes: {req.functional_scope.integracoes}
            Webhooks: {req.functional_scope.webhooks}
            """,
            DiscoveryStage.CONSTRAINTS_POLICIES: f"""
            LGPD PII: {req.constraints_policies.lgpd_pii}
            Seguran√ßa: {req.constraints_policies.seguranca}
            Auditoria: {req.constraints_policies.auditoria}
            Compliance: {req.constraints_policies.compliance}
            """,
            DiscoveryStage.NON_FUNCTIONAL: f"""
            SLOs: {req.non_functional.slos}
            Disponibilidade: {req.non_functional.disponibilidade}
            Custo Alvo: R$ {req.non_functional.custo_alvo}
            Escalabilidade: {req.non_functional.escalabilidade}
            """,
            DiscoveryStage.TECH_PREFERENCES: f"""
            Stacks Permitidas: {req.tech_preferences.stacks_permitidas}
            Stacks Vedadas: {req.tech_preferences.stacks_vedadas}
            Legado: {req.tech_preferences.legado}
            Infraestrutura: {req.tech_preferences.infraestrutura_preferida}
            """,
            DiscoveryStage.DELIVERY_BUDGET: f"""
            Marcos: {req.delivery_budget.marcos}
            Prazos: {req.delivery_budget.prazos}
            Budget: R$ {req.delivery_budget.budget}
            Governan√ßa: {req.delivery_budget.governanca}
            """,
            DiscoveryStage.REVIEW_GAPS: f"""
            Lacunas: {req.review_gaps.lacunas_identificadas}
            Trade-offs: {req.review_gaps.trade_offs}
            Decis√µes Pendentes: {req.review_gaps.decisoes_pendentes}
            Riscos Aceitos: {req.review_gaps.riscos_aceitos}
            """,
        }

        return stage_contexts.get(stage, "Contexto n√£o dispon√≠vel")

    async def _build_files_context(self, uploaded_files: List[Dict[str, Any]]) -> str:
        """Constr√≥i contexto dos arquivos enviados para a IA."""
        context_parts = [
            "=== ARQUIVOS ENVIADOS PELO USU√ÅRIO ===",
            f"Total de arquivos: {len(uploaded_files)}",
            "",
        ]

        for i, file_info in enumerate(uploaded_files, 1):
            file_type = file_info.get("file_type", "unknown")
            filename = file_info.get("original_filename", "unknown")

            context_parts.append(f"ARQUIVO {i}: {filename}")
            context_parts.append(f"Tipo: {file_type}")

            # Adicionar conte√∫do processado se dispon√≠vel
            processed_content = file_info.get("processed_content", {})

            if file_type == "image":
                # Contexto de imagens
                analysis = processed_content.get("ai_analysis", {})
                context_parts.extend(
                    [
                        f"- Elementos UI: {', '.join(analysis.get('ui_elements', [])[:5])}",
                        f"- Padr√µes: {', '.join(analysis.get('design_patterns', [])[:3])}",
                        f"- Estilo: {analysis.get('design_style', 'n√£o identificado')}",
                        f"- Texto extra√≠do: {analysis.get('extracted_text', '')[:100]}...",
                    ]
                )

                # Sugest√µes t√©cnicas
                suggestions = analysis.get("implementation_suggestions", [])
                if suggestions:
                    context_parts.append(f"- Sugest√µes t√©cnicas: {'; '.join(suggestions[:3])}")

            elif file_type == "pdf":
                # Contexto de PDFs
                analysis = processed_content.get("ai_analysis", {})
                structured_data = processed_content.get("structured_data", {})

                context_parts.extend(
                    [
                        f"- Tipo de documento: {analysis.get('document_type', 'n√£o identificado')}",
                        f"- P√°ginas: {structured_data.get('total_pages', 0)}",
                        f"- Cont√©m tabelas: {'Sim' if structured_data.get('has_tables') else 'N√£o'}",
                    ]
                )

                # Requisitos identificados
                req_types = analysis.get("content_types", [])
                if req_types:
                    context_parts.append(f"- Tipos de conte√∫do: {', '.join(req_types[:5])}")

                # Regras de neg√≥cio
                business_rules = analysis.get("business_rules", [])
                if business_rules:
                    context_parts.append(f"- Regras de neg√≥cio: {'; '.join(business_rules[:3])}")

                # Requisitos t√©cnicos
                tech_req = analysis.get("technical_requirements", [])
                if tech_req:
                    context_parts.append(f"- Req. t√©cnicos: {'; '.join(tech_req[:3])}")

            elif file_type == "markdown":
                # Contexto de Markdown
                analysis = processed_content.get("ai_analysis", {})
                structured_data = processed_content.get("structured_data", {})

                context_parts.extend(
                    [
                        f"- Tipo de doc: {analysis.get('doc_type', 'n√£o identificado')}",
                        f"- Headers: {len(structured_data.get('headers', []))}",
                        f"- Blocos de c√≥digo: {len(structured_data.get('code_blocks', []))}",
                    ]
                )

                # Requisitos existentes
                existing_req = analysis.get("existing_requirements", [])
                if existing_req:
                    context_parts.append(
                        f"- Requisitos identificados: {'; '.join(existing_req[:3])}"
                    )

                # Tecnologias mencionadas
                dependencies = analysis.get("dependencies", [])
                if dependencies:
                    context_parts.append(f"- Tecnologias: {', '.join(dependencies[:5])}")

            # Insights gerais
            insights = processed_content.get("insights", [])
            if insights:
                context_parts.append(f"- Insights: {'; '.join(insights[:2])}")

            context_parts.append("")  # Linha em branco entre arquivos

        context_parts.append("=== FIM DOS ARQUIVOS ===")
        context_parts.append("")
        context_parts.append(
            "INSTRU√á√ïES: Use essas informa√ß√µes dos arquivos para enriquecer suas perguntas e sugest√µes. Referencie elementos espec√≠ficos dos arquivos quando relevante para o est√°gio atual da descoberta."
        )

        return "\n".join(context_parts)

    async def _prepare_multimodal_messages(
        self, messages: List[Dict], uploaded_files: List[Dict[str, Any]]
    ) -> List[Dict]:
        """
        Prepara mensagens com conte√∫do multimodal para OpenAI GPT-4 Vision.
        Processa imagens, PDFs e documentos para incluir nas mensagens.
        """
        try:
            # Encontrar a √∫ltima mensagem do usu√°rio para adicionar conte√∫do multimodal
            user_message_idx = None
            for i in range(len(messages) - 1, -1, -1):
                if messages[i]["role"] == "user":
                    user_message_idx = i
                    break

            if user_message_idx is None:
                return messages

            user_message = messages[user_message_idx]
            multimodal_content = []

            # Adicionar texto original
            multimodal_content.append({"type": "text", "text": user_message["content"]})

            # Processar cada arquivo enviado
            for file_info in uploaded_files:
                file_type = file_info.get("file_type", "unknown")
                processed_content = file_info.get("processed_content", {})

                if file_type == "image":
                    # Para imagens, usar GPT-4 Vision
                    image_url = file_info.get("processed_content", {}).get("base64_data")
                    if image_url:
                        multimodal_content.append(
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_url}",
                                    "detail": "high",
                                },
                            }
                        )

                elif file_type == "pdf":
                    # Para PDFs, adicionar texto extra√≠do e an√°lise
                    pdf_analysis = processed_content.get("ai_analysis", {})
                    extracted_text = processed_content.get("extracted_text", "")

                    pdf_context = f"""
üìÑ **CONTE√öDO DO PDF ENVIADO:**

**Tipo:** {pdf_analysis.get('document_type', 'Documento t√©cnico')}
**P√°ginas:** {processed_content.get('structured_data', {}).get('total_pages', 'N/A')}

**Regras de Neg√≥cio Identificadas:**
{chr(10).join(f'‚Ä¢ {rule}' for rule in pdf_analysis.get('business_rules', [])[:5])}

**Requisitos T√©cnicos:**
{chr(10).join(f'‚Ä¢ {req}' for req in pdf_analysis.get('technical_requirements', [])[:5])}

**Texto Relevante:**
{extracted_text[:1000]}...
"""

                    multimodal_content.append({"type": "text", "text": pdf_context})

                elif file_type == "markdown":
                    # Para Markdown, adicionar conte√∫do estruturado
                    md_analysis = processed_content.get("ai_analysis", {})

                    md_context = f"""
üìù **DOCUMENTA√á√ÉO MARKDOWN ENVIADA:**

**Tipo:** {md_analysis.get('doc_type', 'Documenta√ß√£o t√©cnica')}

**Requisitos Existentes:**
{chr(10).join(f'‚Ä¢ {req}' for req in md_analysis.get('existing_requirements', [])[:5])}

**Tecnologias Mencionadas:**
{chr(10).join(f'‚Ä¢ {tech}' for tech in md_analysis.get('dependencies', [])[:8])}

**Insights:**
{chr(10).join(f'‚Ä¢ {insight}' for insight in processed_content.get('insights', [])[:3])}
"""

                    multimodal_content.append({"type": "text", "text": md_context})

            # Atualizar mensagem do usu√°rio com conte√∫do multimodal
            messages[user_message_idx] = {"role": "user", "content": multimodal_content}

            return messages

        except Exception as e:
            logger.error("Erro ao preparar mensagens multimodais: {}", str(e))
            return messages  # Fallback para mensagens originais
